
toDO# add keyword search or elastic search to rag and gag
done# return chunks based on vector searched nodes/edges and for top nodes

done#make function in networkX to get dataframe with global_contextual_proximity
done#concatenate third dataframe with two before - maybe only melt the third with first

toDO# preprocess data_input (get rid of headers etc)
toDO# implement keyword search next to vector search for text

toDO#evaluate: concatenated dataframes currently with all three - better only dfg1 and dfg_global?
done# weights from chunks and globals are double weighted  - weight adjusted of global prox to 0.5
done# evaluate -> weight adjustment?, normalization?, use only one?
toDO# adjust self-join from global_prox to not only count pairs over dataset with all possible pairs but pairs, that are somewhat close to each other?

toDO#make KG with HI splitted and global proximity
toDO#use and respect weights from KG for GAG

done# in GAG - not only retrieving single node1 - relation - node2 -> but rather the next k-nodes as well based on weights
done# follow up: biggest weight usually the biggest ones (arbeitnehmer - arbeitgeber) - some kind of tf-idf?
done# tf-idf too heavy -implement a weighted combination of TF-IDF score and edge weight
toDO# evaluate the weighted combination of tf-idf and weights - tweak the factors

toDO# store the KG vectorstore properly so its not getting created every run

partially-done# apply traveling salesman problam (-variant) for all vectorsearched nodes
toDO: tsp doesnt seem to properly work
done# apply some sort dijkstra algo on any query-vectorsearched-node to find all ways in between them (but with inverted weights cuz they are positive)
done# Topic Clustering: Instead of just looking at paths, consider clustering similar nodes based on their contextual relationships.
This can provide a broader overview of the topic.
done# get node-edge-node triplets based on clustered topics
toDO# dijkstra not giving relevant nodes?
toDO# evaluate issue with top-n approach based on weights of edges - again some sort of tf-idf approach?

done# get contextual nodes as well for custom top_nodes
toDO# evaluate adding contextual nodes for custom top_nodes

done# Custom Algorithm for Context Generation: Develop a custom algorithm that selects a set of nodes and edges based on their
relevance and informativeness regarding the query, rather than just their proximity.
-> combined scores of cos score, tf-idf score and centralized score



Retreiver:

#toDO: embed children (chunk size maybe 512, 256) and retreive but give HI as response
#toDO: chunk granularity paper vs. semantic chunking llama pack vs normal splitters
#toDO: retrieve query type with facts and dynamically set system prompts based on that
#toDO: other retrieval methods?
#toDO: reranker as well?